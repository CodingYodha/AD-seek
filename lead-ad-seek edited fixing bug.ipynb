{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12142916,"sourceType":"datasetVersion","datasetId":7647679}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nimport matplotlib.pyplot as plt\nimport gc # Garbage Collector interface\n\n# ===================================================================\n# SETUP\n# ===================================================================\n# Set random seeds for reproducibility\ntorch.manual_seed(42)\nnp.random.seed(42)\n\n# Device configuration\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# ===================================================================\n# DATA LOADING & PREPARATION\n# ===================================================================\nprint(\"\\nLoading dataset for GAN training...\")\ntry:\n    # Load only the data needed for this step\n    with np.load(\"/kaggle/input/largest-alzheimer-eeg-dataset/integrated_eeg_dataset.npz\") as data:\n        X_raw = data['X_raw']\n        y_labels = data['y_labels']\n    \n    y = y_labels[:, 0].astype(float).astype(int)\n    print(f\"- X_raw shape: {X_raw.shape}\")\n    print(f\"- y_labels shape: {y_labels.shape}\")\n\nexcept Exception as e:\n    print(f\"Error loading dataset: {e}\")\n    raise\n\n# --- Memory-Efficient Normalization ---\nprint(\"Normalizing data...\")\n# Normalize each channel separately. This is done in-place on a copy.\nX_normalized = np.empty_like(X_raw, dtype=np.float32)\nfor c in range(X_raw.shape[2]):\n    min_val = X_raw[:, :, c].min()\n    max_val = X_raw[:, :, c].max()\n    X_normalized[:, :, c] = 2 * ((X_raw[:, :, c] - min_val) / (max_val - min_val)) - 1\n\n# --- Explicitly free memory ---\nprint(\"Freeing original raw data from memory...\")\ndel X_raw\ngc.collect()\n\n# Convert to PyTorch tensors\nX_tensor = torch.from_numpy(X_normalized)\ny_tensor = torch.from_numpy(y) # Not used for GAN, but good practice\n\n# Create dataset and dataloader\nbatch_size = 256\ndataset = TensorDataset(X_tensor) # We don't need labels for unconditional GAN\n# --- OPTIMIZED DATALOADER ---\ndataloader = DataLoader(\n    dataset,\n    batch_size=batch_size,\n    shuffle=True,\n    num_workers=4,      # Use multiple CPU cores to load data in parallel\n    pin_memory=True,    # Speeds up CPU to GPU data transfer\n    persistent_workers=True # Keeps workers alive, avoids re-creation overhead\n)\n\n# Free more memory\ndel X_normalized, X_tensor, y_tensor\ngc.collect()\nprint(\"Prepared DataLoader and freed intermediate arrays.\")\n\n# ===================================================================\n# GAN MODEL DEFINITION (No changes needed here)\n# ===================================================================\nclass Generator(nn.Module):\n    def __init__(self, latent_dim, output_shape):\n        super(Generator, self).__init__()\n        self.output_shape = output_shape\n        self.model = nn.Sequential(\n            nn.Linear(latent_dim, 128), nn.LeakyReLU(0.2), nn.BatchNorm1d(128),\n            nn.Linear(128, 256), nn.LeakyReLU(0.2), nn.BatchNorm1d(256),\n            nn.Linear(256, 512), nn.LeakyReLU(0.2), nn.BatchNorm1d(512),\n            nn.Linear(512, np.prod(output_shape)), nn.Tanh()\n        )\n    def forward(self, z):\n        return self.model(z).view(-1, *self.output_shape)\n\nclass Discriminator(nn.Module):\n    def __init__(self, input_shape):\n        super(Discriminator, self).__init__()\n        self.model = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(np.prod(input_shape), 512), nn.LeakyReLU(0.2),\n            nn.Linear(512, 256), nn.LeakyReLU(0.2),\n            nn.Linear(256, 1), nn.Sigmoid()\n        )\n    def forward(self, x):\n        return self.model(x)\n\n# ===================================================================\n# GAN TRAINING (No changes needed in the loop logic)\n# ===================================================================\ntimesteps, channels = 128, 19 # Hardcode from dataset info\nlatent_dim = 100\nnum_epochs = 60 # Consider reducing for faster runs if needed\n\ngenerator = Generator(latent_dim, (timesteps, channels)).to(device)\ndiscriminator = Discriminator((timesteps, channels)).to(device)\n\ng_optimizer = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\nd_optimizer = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\ncriterion = nn.BCELoss()\n\nprint(\"\\nStarting GAN training...\")\ng_losses, d_losses = [], []\nfor epoch in range(num_epochs):\n    for i, (real_samples,) in enumerate(dataloader):\n        batch_size_real = real_samples.size(0)\n        real_samples = real_samples.to(device)\n        real_labels = torch.ones(batch_size_real, 1).to(device)\n        fake_labels = torch.zeros(batch_size_real, 1).to(device)\n        \n        # Train Discriminator\n        d_optimizer.zero_grad()\n        d_loss = criterion(discriminator(real_samples), real_labels) + \\\n                 criterion(discriminator(generator(torch.randn(batch_size_real, latent_dim).to(device)).detach()), fake_labels)\n        d_loss.backward()\n        d_optimizer.step()\n        \n        # Train Generator\n        g_optimizer.zero_grad()\n        g_loss = criterion(discriminator(generator(torch.randn(batch_size_real, latent_dim).to(device))), real_labels)\n        g_loss.backward()\n        g_optimizer.step()\n\n    g_losses.append(g_loss.item())\n    d_losses.append(d_loss.item())\n    if (epoch + 1) % 20 == 0:\n        print(f\"Epoch [{epoch+1}/{num_epochs}], D Loss: {d_loss.item():.4f}, G Loss: {g_loss.item():.4f}\")\n\nplt.figure(figsize=(10, 5))\nplt.plot(d_losses, label=\"Discriminator Loss\")\nplt.plot(g_losses, label=\"Generator Loss\")\nplt.title(\"GAN Training Losses\")\nplt.legend()\nplt.savefig(\"gan_training_losses.png\")\nplt.show()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T17:54:05.152844Z","iopub.execute_input":"2025-06-12T17:54:05.153250Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n\nLoading dataset for GAN training...\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# ===================================================================\n# GENERATE & SAVE SYNTHETIC SAMPLES (Memory-Efficient)\n# ===================================================================\ndef generate_samples(generator, num_samples, latent_dim, device):\n    generator.eval()\n    with torch.no_grad():\n        z = torch.randn(num_samples, latent_dim).to(device)\n        return generator(z).cpu().numpy()\n\n# Determine samples to generate to balance the dataset\nmin_value = min(y)  # Find lowest label value\ny_shifted = y - min_value  # Shift all values to be non-negative\nclass_counts = np.bincount(y_shifted)  # Works now!\n\nmax_count = np.max(class_counts)\nsamples_to_generate = max_count - class_counts\n\nprint(\"\\nOriginal class distribution:\")\nfor i, count in enumerate(class_counts):\n    print(f\"Class {i}: {count} samples\")\n\nsynthetic_samples_list = []\nsynthetic_labels_list = []\n\nfor class_idx, num_to_gen in enumerate(samples_to_generate):\n    if num_to_gen > 0:\n        print(f\"Generating {num_to_gen} synthetic samples for class {class_idx}...\")\n        # Note: This is an unconditional GAN, so we generate data and assign labels.\n        # For a conditional GAN (cGAN), you would feed the class_idx to the generator.\n        generated_data = generate_samples(generator, num_to_gen, latent_dim, device)\n        synthetic_samples_list.append(generated_data)\n\n        # Create corresponding labels\n        generated_labels = np.zeros((num_to_gen, y_labels.shape[1]), dtype=object)\n        generated_labels[:, 0] = class_idx  # Class label\n        generated_labels[:, 1] = -1         # Subject ID (-1 for synthetic)\n        generated_labels[:, 2] = \"synthetic\"  # Dataset name\n        synthetic_labels_list.append(generated_labels)\n\n# --- Combine ONLY the newly generated samples ---\nif synthetic_samples_list:\n    X_synthetic = np.concatenate(synthetic_samples_list, axis=0)\n    y_synthetic = np.concatenate(synthetic_labels_list, axis=0)\n    print(f\"\\nGenerated a total of {X_synthetic.shape[0]} synthetic samples.\")\nelse:\n    # Handle the case where the dataset is already balanced\n    X_synthetic = np.array([])\n    y_synthetic = np.array([])\n    print(\"\\nDataset is already balanced. No new samples generated.\")\n\n# --- Save original and synthetic data separately to avoid large concatenation ---\ntry:\n    print(\"Reloading original data to save alongside synthetic data...\")\n    with np.load(\"/kaggle/input/largest-alzheimer-eeg-dataset/integrated_eeg_dataset.npz\") as data:\n        X_original = data['X_raw']\n        y_original = data['y_labels']\n\n    save_path = \"/kaggle/working/augmented_eeg_dataset.npz\"\n    np.savez_compressed(\n        save_path,\n        X_original=X_original,\n        y_original=y_original,\n        X_synthetic=X_synthetic,\n        y_synthetic=y_synthetic\n    )\n    print(f\"\\nAugmented dataset components saved successfully to {save_path}\")\n    print(f\"File contains: X_original, y_original, X_synthetic, y_synthetic\")\n\nexcept Exception as e:\n    print(f\"Error saving augmented dataset: {e}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.signal import welch\nfrom scipy.integrate import simpson as simps\nimport gc\n\n# ===================================================================\n# SETUP\n# ===================================================================\ntorch.manual_seed(42)\nnp.random.seed(42)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ===================================================================\n# BATCHED FEATURE EXTRACTION (MEMORY EFFICIENT)\n# ===================================================================\ndef extract_enhanced_features(X_batch, sfreq=128):\n    \"\"\"\n    Extracts features for a single batch of data.\n    X_batch shape: [batch_size, n_timesteps, n_channels]\n    \"\"\"\n    bands = {'delta': (0.5, 4), 'theta': (4, 8), 'alpha': (8, 13), 'beta': (13, 30), 'gamma': (30, 50)}\n    nperseg = X_batch.shape[1]\n    \n    freqs, psd = welch(X_batch, fs=sfreq, nperseg=nperseg, axis=1)\n    psd = np.transpose(psd, (0, 2, 1)) # Transpose to (batch, channels, freqs)\n    \n    all_band_powers = []\n    freq_res = freqs[1] - freqs[0]\n    for band_limits in bands.values():\n        band_idx = np.logical_and(freqs >= band_limits[0], freqs <= band_limits[1])\n        bp = simps(psd[:, :, band_idx], dx=freq_res, axis=-1)\n        all_band_powers.append(bp)\n        \n    abs_bp = np.stack(all_band_powers, axis=-1)\n    total_power = abs_bp.sum(axis=-1, keepdims=True)\n    total_power[total_power == 0] = 1e-10\n    rel_bp = abs_bp / total_power\n    \n    features = np.concatenate([\n        rel_bp.reshape(X_batch.shape[0], -1),\n        abs_bp.reshape(X_batch.shape[0], -1),\n        X_batch.mean(axis=1),\n        X_batch.std(axis=1),\n        X_batch.max(axis=1),\n        X_batch.min(axis=1)\n    ], axis=1)\n    return features\n\ndef extract_features_in_batches(X_raw, batch_size=1024):\n    \"\"\"\n    Processes a large raw data array in smaller batches to conserve memory.\n    \"\"\"\n    n_samples = X_raw.shape[0]\n    all_features = []\n    print(f\"Extracting features for {n_samples} samples in batches of {batch_size}...\")\n    for i in range(0, n_samples, batch_size):\n        batch_end = min(i + batch_size, n_samples)\n        X_batch = X_raw[i:batch_end]\n        \n        features_batch = extract_enhanced_features(X_batch)\n        all_features.append(features_batch)\n        \n        if (i // batch_size) % 10 == 0:\n            print(f\"  Processed {batch_end}/{n_samples} samples...\")\n    \n    print(\"Feature extraction complete.\")\n    return np.concatenate(all_features, axis=0)\n\n# ===================================================================\n# MODEL, TRAINING, AND EVALUATION FUNCTIONS (No changes needed)\n# ===================================================================\nclass EnhancedEEGCNN(nn.Module):\n    def __init__(self, input_features, num_classes=2):\n        super(EnhancedEEGCNN, self).__init__()\n        # Adjusting the architecture for 1D feature vectors\n        self.features = nn.Sequential(\n            nn.Conv1d(1, 32, kernel_size=3, padding=1), nn.BatchNorm1d(32), nn.ELU(), nn.MaxPool1d(2),\n            nn.Conv1d(32, 64, kernel_size=3, padding=1), nn.BatchNorm1d(64), nn.ELU(), nn.MaxPool1d(2),\n            nn.Conv1d(64, 128, kernel_size=3, padding=1), nn.BatchNorm1d(128), nn.ELU(), nn.MaxPool1d(2),\n        )\n        # Determine the flattened size dynamically\n        dummy_input = torch.randn(1, 1, input_features)\n        flat_size = self.features(dummy_input).view(1, -1).shape[1]\n        \n        self.classifier = nn.Sequential(\n            nn.Flatten(), nn.Linear(flat_size, 128), nn.Dropout(0.5), nn.ELU(), nn.Linear(128, num_classes)\n        )\n    def forward(self, x):\n        return self.classifier(self.features(x.unsqueeze(1)))\n\ndef train_model(model, train_loader, test_loader, num_epochs=30, model_save_path='best_model.pth'):\n    model = model.to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n    best_acc = 0.0\n    \n    for epoch in range(num_epochs):\n        model.train()\n        for inputs, labels in train_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n        \n        val_acc = evaluate_accuracy(model, test_loader)\n        if val_acc > best_acc:\n            best_acc = val_acc\n            torch.save(model.state_dict(), model_save_path)\n        if (epoch+1) % 5 == 0:\n          print(f\"Epoch {epoch+1}/{num_epochs}, Val Acc: {val_acc:.4f}\")\n    \n    print(f\"Training finished. Best validation accuracy: {best_acc:.4f}\")\n    model.load_state_dict(torch.load(model_save_path)) # Load best model\n    return model\n\ndef evaluate_accuracy(model, loader):\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for inputs, labels in loader:\n            outputs = model(inputs.to(device))\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels.to(device)).sum().item()\n    return correct / total\n\ndef full_evaluation(model, loader, dataset_name):\n    model.eval()\n    all_preds, all_labels = [], []\n    with torch.no_grad():\n        for inputs, labels in loader:\n            outputs = model(inputs.to(device))\n            _, preds = torch.max(outputs, 1)\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.numpy())\n            \n    print(f\"\\n--- Full Evaluation on {dataset_name} ---\")\n    print(classification_report(all_labels, all_preds, target_names=['Class 0', 'Class 1']))\n    cm = confusion_matrix(all_labels, all_preds)\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues').set_title(f'Confusion Matrix ({dataset_name})')\n    plt.show()\n\n# ===================================================================\n# MAIN EXECUTION PIPELINE\n# ===================================================================\nif __name__ == \"__main__\":\n    original_dataset_path = \"/kaggle/input/largest-alzheimer-eeg-dataset/integrated_eeg_dataset.npz\"\n    augmented_dataset_path = \"/kaggle/working/augmented_eeg_dataset.npz\"\n\n    # --- Step 1: Train on Original Data ---\n    print(\"\\n\" + \"=\"*50)\n    print(\"STEP 1: PROCESSING AND TRAINING ON ORIGINAL DATASET\")\n    print(\"=\"*50)\n\n    print(\"\\nLoading and CLEANING original dataset...\")\n    with np.load(original_dataset_path) as data:\n        X_orig_raw = data['X_raw']\n        # Load labels as strings to handle '0.0' etc.\n        y_labels_str = data['y_labels'][:, 0]\n    \n    # --- NEW ROBUST FIX: CLEANING AND FILTERING ---\n    \n    # 1. Define which string labels belong to our two target classes\n    class_0_labels = ['0', '0.0']\n    class_1_labels = ['1', '1.0']\n    \n    # 2. Create a boolean mask to select ONLY the samples from our target classes\n    mask_class_0 = np.isin(y_labels_str, class_0_labels)\n    mask_class_1 = np.isin(y_labels_str, class_1_labels)\n    valid_mask = mask_class_0 | mask_class_1\n    \n    print(f\"Original total sample count: {len(y_labels_str)}\")\n    print(f\"Found {np.sum(valid_mask)} valid samples for binary classification (Classes 0 & 1).\")\n    print(f\"Discarding {len(y_labels_str) - np.sum(valid_mask)} samples with labels like '-1' and '2'.\")\n    \n    # 3. Filter both the X and y data to keep only valid samples\n    X_orig_clean = X_orig_raw[valid_mask]\n    y_labels_filtered = y_labels_str[valid_mask]\n    \n    # 4. Create the final clean integer label array (guaranteed to be 0s and 1s)\n    # Start with all zeros\n    y_orig_labels = np.zeros(len(y_labels_filtered), dtype=int)\n    # Find where the original labels were for class 1, and set those indices to 1\n    y_orig_labels[np.isin(y_labels_filtered, class_1_labels)] = 1\n    \n    print(f\"Final clean dataset size: {X_orig_clean.shape[0]} samples.\")\n    print(f\"Clean class distribution: Class 0: {np.sum(y_orig_labels == 0)}, Class 1: {np.sum(y_orig_labels == 1)}\")\n    \n    # --- END OF FIX ---\n    \n    # Now, proceed with the perfectly clean data\n    X_orig_features = extract_features_in_batches(X_orig_clean)\n    del X_orig_clean, X_orig_raw, y_labels_str, y_labels_filtered; gc.collect()  # Free memory\n\n\n\n    X_orig_tensor = torch.FloatTensor(X_orig_features)\n    y_orig_tensor = torch.LongTensor(y_orig_labels)\n    del X_orig_features, y_orig_labels; gc.collect()\n    \n    X_train, X_test, y_train, y_test = train_test_split(\n        X_orig_tensor, y_orig_tensor, test_size=0.2, random_state=42, stratify=y_orig_tensor)\n\n    orig_train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=64, shuffle=True)\n    orig_test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=64, shuffle=False)\n    del X_train, X_test, y_train, y_test, X_orig_tensor, y_orig_tensor; gc.collect()\n\n    input_size = orig_train_loader.dataset.tensors[0].shape[1]\n    model_orig = EnhancedEEGCNN(input_features=input_size)\n    print(\"\\nTraining model on original data...\")\n    model_orig = train_model(model_orig, orig_train_loader, orig_test_loader, num_epochs=30, model_save_path='model_orig.pth')\n\n\n    # --- Step 2: Train on Augmented Data ---\n    print(\"\\n\" + \"=\"*50)\n    print(\"STEP 2: PROCESSING AND TRAINING ON AUGMENTED DATASET\")\n    print(\"=\"*50)\n\n# In STEP 2: PROCESSING AND TRAINING ON AUGMENTED DATASET\nprint(\"\\nLoading and CLEANING augmented dataset...\")\nwith np.load(augmented_dataset_path) as data:\n    # Load all components\n    X_original_raw = data['X_original']\n    y_original_labels_str = data['y_original'][:, 0]\n    X_synthetic_raw = data['X_synthetic']\n    y_synthetic_labels_str = data['y_synthetic'][:, 0]\n\n    # Combine original and synthetic data\n    X_aug_raw = np.concatenate((X_original_raw, X_synthetic_raw), axis=0)\n    y_aug_labels_str = np.concatenate((y_original_labels_str, y_synthetic_labels_str), axis=0)\n    \n    del X_original_raw, y_original_labels_str, X_synthetic_raw, y_synthetic_labels_str\n    gc.collect()\n\n    # --- NEW ROBUST FIX FOR AUGMENTED DATA ---\n    \n    # 1. Define which string labels belong to our two target classes\n    class_0_labels = ['0', '0.0']\n    class_1_labels = ['1', '1.0']\n    \n    # 2. Create a boolean mask to select ONLY the samples from our target classes\n    mask_class_0 = np.isin(y_aug_labels_str, class_0_labels)\n    mask_class_1 = np.isin(y_aug_labels_str, class_1_labels)\n    valid_mask = mask_class_0 | mask_class_1\n    \n    print(f\"Augmented total sample count: {len(y_aug_labels_str)}\")\n    print(f\"Found {np.sum(valid_mask)} valid samples for binary classification (Classes 0 & 1).\")\n    print(f\"Discarding {len(y_aug_labels_str) - np.sum(valid_mask)} samples with labels like '-1' and '2'.\")\n    \n    # 3. Filter both the X and y data to keep only valid samples\n    X_aug_clean = X_aug_raw[valid_mask]\n    y_labels_filtered = y_aug_labels_str[valid_mask]\n    \n    # 4. Create the final clean integer label array (guaranteed to be 0s and 1s)\n    y_aug_labels = np.zeros(len(y_labels_filtered), dtype=int)\n    y_aug_labels[np.isin(y_labels_filtered, class_1_labels)] = 1\n    \n    print(f\"Final clean augmented dataset size: {X_aug_clean.shape[0]} samples.\")\n    print(f\"Clean augmented class distribution: Class 0: {np.sum(y_aug_labels == 0)}, Class 1: {np.sum(y_aug_labels == 1)}\")\n    \n    # --- END OF FIX ---\n    \n    # Now, proceed with the perfectly clean augmented data\n    X_aug_features = extract_features_in_batches(X_aug_clean)\n    del X_aug_clean, X_aug_raw, y_aug_labels_str, y_labels_filtered; gc.collect()\n    X_aug_features = extract_features_in_batches(X_aug_raw)\n    del X_aug_raw; gc.collect()\n\n    X_aug_tensor = torch.FloatTensor(X_aug_features)\n    y_aug_tensor = torch.LongTensor(y_aug_labels)\n    del X_aug_features, y_aug_labels; gc.collect()\n\n    X_train, X_test, y_train, y_test = train_test_split(\n        X_aug_tensor, y_aug_tensor, test_size=0.2, random_state=42, stratify=y_aug_tensor)\n    \n    aug_train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=64, shuffle=True)\n    aug_test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=64, shuffle=False)\n    del X_train, X_test, y_train, y_test, X_aug_tensor, y_aug_tensor; gc.collect()\n\n    model_aug = EnhancedEEGCNN(input_features=input_size)\n    print(\"\\nTraining model on augmented data...\")\n    model_aug = train_model(model_aug, aug_train_loader, aug_test_loader, num_epochs=30, model_save_path='model_aug.pth')\n\n    # --- Step 3: Final Evaluation ---\n    print(\"\\n\" + \"=\"*50)\n    print(\"STEP 3: FINAL COMPARATIVE EVALUATION\")\n    print(\"=\"*50)\n    \n    full_evaluation(model_orig, orig_test_loader, \"Original Model on Original Test Set\")\n    full_evaluation(model_aug, aug_test_loader, \"Augmented Model on Augmented Test Set\")\n    \n    # Cross-evaluation\n    print(\"\\n--- Cross-Evaluation ---\")\n    full_evaluation(model_orig, aug_test_loader, \"Original Model on Augmented Test Set\")\n    full_evaluation(model_aug, orig_test_loader, \"Augmented Model on Original Test Set\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport os\nimport humanize\nfrom datetime import datetime\n\ndef analyze_npz_file(file_path):\n    \"\"\"Comprehensive analysis of an NPZ file\"\"\"\n    if not os.path.exists(file_path):\n        print(f\"Error: File {file_path} not found\")\n        return\n    \n    # Get file metadata\n    file_size = os.path.getsize(file_path)\n    mod_time = os.path.getmtime(file_path)\n    \n    print(f\"=== File Analysis: {os.path.basename(file_path)} ===\")\n    print(f\"Path: {os.path.abspath(file_path)}\")\n    print(f\"Size: {humanize.naturalsize(file_size)}\")\n    print(f\"Modified: {datetime.fromtimestamp(mod_time).strftime('%Y-%m-%d %H:%M:%S')}\")\n    print(\"\\nContents:\")\n    \n    try:\n        # Load the npz file\n        with np.load(file_path, allow_pickle=True) as data:\n            # Print array information\n            for array_name in data.files:\n                array = data[array_name]\n                print(f\"\\nArray: {array_name}\")\n                print(f\"  Shape: {array.shape}\")\n                print(f\"  Data type: {array.dtype}\")\n                print(f\"  Memory size: {humanize.naturalsize(array.nbytes)}\")\n                \n                # Additional info for label arrays\n                if array_name == 'y_labels':\n                    unique_labels = np.unique(array[:, 0])\n                    print(f\"  Unique labels: {unique_labels}\")\n                    print(f\"  Label distribution:\")\n                    for label in unique_labels:\n                        count = np.sum(array[:, 0] == label)\n                        print(f\"    Label {label}: {count} samples ({count/len(array)*100:.1f}%)\")\n                \n                # Sample data preview\n                if len(array.shape) == 1:\n                    print(f\"  Sample values: {array[:5]}\")\n                elif len(array.shape) == 2:\n                    print(f\"  Sample row: {array[0, :5]}...\")\n                elif len(array.shape) == 3:\n                    print(f\"  Sample data: {array[0, 0, :5]}...\")\n                \n                # Memory optimization suggestions\n                if array.dtype == 'float64':\n                    print(\"  Note: Consider float32 for memory savings\")\n    \n    except Exception as e:\n        print(f\"Error analyzing file: {e}\")\n\n# Analyze your specific file\nif __name__ == \"__main__\":\n    file_path = \"/kaggle/input/largest-alzheimer-eeg-dataset/integrated_eeg_dataset.npz\"\n    analyze_npz_file(file_path)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}